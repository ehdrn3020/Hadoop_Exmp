#위 그림
에서와 같이 Flume Agent를 바탕으로 Complex flow를 만들수 있다. 이른바, Fan-in, Fan-out한 구조인데,

Spooling Directory Source : WAS의 Log 파일 위치의 디렉토리를 Spooling하여 파일이 만들어졌을때를 모니터링하여 수집
Thrift Source : WAS에서 로그를 파일로 별도로 남기지 않고, Thrift 통신으로 직접 Agent에 발송하여 로그 수집
Exec Source : WAS의 로그파일을 tailing하여 로그 수집
Avro Source : 각 WAS에서 수집된 로그를 Avro 통신을 통해 로그를 중간 집적서버로 전송, 
              Avro Source를 통해 들어온 이벤트를 Channel selector를 통해 각각의 Channel로 전송

Avro Sink : 또 다른 Agent 서버로 이벤트를 Avro 통신을 통해 전송하여, 해당 Agent는 Kafka Sink를 통해 분산 메시징 큐인 Kafka로 저장하여, 
            필요한 Application에서 메시지 가져갈 수 있게 준비함. 일반적으로 실시간 처리를 하기 위해 많이 활용된다.
Elasticsearch Sink : 오픈소스 검색엔진인 Elastic-Search 로 Sink하여 Kibana와 같은 로그 통계,모니터링을 통해 서비스 가능
Hadoop Sink : 대용량 분산 저장을 위해 HDFS(Hadoop Distributed File System))로 저장하여, Hive,Pig,R,Mahout 등으로 
              배치 분석,기계어 학습 등에 활용
참고로 2->4->5의 Flow를 통해 CEP 아키텍처 기반의 실시간 처리가 가능한 엔진으로 전달하면 Realtime 수집,분석을 할 수 있다. 



#수집상태 모니터링

Flume 을 통해 수집상태를 모니터링하고자 할때 Flume은 아래 타입들이 지원된다.

@ Ganglia
Ganglia Monitoring Daemon (gmond)
-Dflume.monitoring.type=gangla 등 몇가지 Property 적용으로 가능
– Ganglia Monitoring System : http://ganglia.sourceforge.net/ 

@ internal HTTP server
위 Start Shell을 자세히 보면 “-Dflume.monitoring.type=http -Dflume.monitoring.port=12345” 값이 있다.
내장된 http 서버로 해당 포트를 통해 JSON형태의 정보가 제공된다.
예를들어 http://localhost:12345/metrics 호출하면 아래와 같은 정보가 보인다.


Ref : http://hochul.net/blog/apache-flume-data-collector_2/
